{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 공식 사이트와 PyTorch로 시작하는 딥러닝 입문을 토대로 공부하였습니다.\n",
    "\n",
    "PyTorch 공식 사이트: https://tutorials.pytorch.kr/beginner/basics/quickstart_tutorial.html\n",
    "\n",
    "PyTorch로 시작하는 딥러닝 입문:https://wikidocs.net/57168"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor\n",
    "\n",
    "PyTorch는 Tensor라는 특수한 자료구조를 사용하는데, 배열과 행렬이랑 매우 비슷하다.\n",
    "\n",
    "PyTorch에서는 Tensor를 사용하여 모델의 입력(input)과 출력(output), 그리고 모델의 매개변수들을 부호화(encode)한다.\n",
    "\n",
    "Tensor는 Numpy의 ndarray와 유사하고, 자동 미분에 최적화되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor 초기화\n",
    "\n",
    "Tensor는 데이터로부터 직접 생성할 수도 있고, NumPy 배열로부터 생성할 수도 있다.\n",
    "\n",
    "데이터로부터 직접 Tensor를 생성하면, 데이터의 자료형은 자동으로 유추한다.\n",
    "\n",
    "NumPy 배열을 이용하여 Tensor를 생성할 수도 있지만, 그 반대인 Tensor를 통해서 NumPy 배열을 생성할 수도 있다.\n",
    "\n",
    "Tensor에서 dim = 0은 행 차원의 제거이고, dim = 1 or -1은 열 차원의 제거이다. \n",
    "\n",
    "즉, dim = 0은 가로 행기준 연산, dim = 1 or -1은 세로 열기준 연산이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터로부터 직접 생성하기\n",
    "\n",
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NumPy 배열로부터 생성하기\n",
    "\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "명시적으로 재정의(override)하지 않는다면, 인자로 주어진 Tensor의 속성을 유지한다.\n",
    "\n",
    "즉, 주어진 텐서와 같은 모양과 자료형을 계속 유지한다는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.5166, 0.6123],\n",
      "        [0.0686, 0.7618]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0., 0.],\n",
      "        [0., 0.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # x_data와 같은 속성을 유지하면서 값을 1로 채우는 것\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # x_data와 같은 속성을 유지하면서 값을 랜덤하게 채우는 것\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "\n",
    "x_zero = torch.zeros_like(x_data, dtype=torch.float) # x_data와 같은 속성을 유지하면서 값을 0으로 채우는 것\n",
    "print(f\"Random Tensor: \\n {x_zero} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존에 존하는 Tensor의 속성을 이용하여 데이터를 편집할 수도 있지만, 처음부터 Tensor의 차원을 결정하여서 만들 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.7570, 0.7762, 0.9068],\n",
      "        [0.8874, 0.4538, 0.8843]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor의 속성(Attribute)\n",
    "\n",
    "Tensor의 속성을 확인하는 방법은 3가지가 있다.\n",
    "\n",
    "shape, dtype, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor의 연산(Operation)\n",
    "\n",
    "기본적으로 Tensor의 연산은 CPU와 GPU를 이용하여서 할 수 있다.\n",
    "\n",
    "일반적으로 GPU가 더 빠른 연산을 가능하게 해주기 때문에 GPU로 Tensor를 이동시켜 연산하는 방법을 사용하면 더 빠른 연산이 가능하다.\n",
    "\n",
    "Tensor를 만들면 기본적으로 CPU에 생성된다. 그렇기 때문에 .to 메소드를 사용하여 GPU로 이동을 시켜야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU가 존재하면 텐서를 이동합니다\n",
    "# 아쉽게도 내 노트북에서는 GPU가 없다....\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor의 인덱싱과 슬라이싱은 NumPy와 매우 비슷하다.\n",
    "\n",
    "그렇기 때문에 Numpy를 사용하듯이 사용하면 거의 다 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([1., 1., 1., 1.])\n",
      "First column:  tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4) #1로 채워져있는 4x4의 tensor 생성\n",
    "\n",
    "print('First row: ', tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column:', tensor[..., -1])\n",
    "tensor[:,1] = 0 # 2번째 column을 0으로 채우기\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor 합치기\n",
    "\n",
    "Tensor를 합치는 방법에는 torch.cat과 torch.stack이 존재한다.\n",
    "\n",
    "두가지 방법은 미묘하게 다른데, torch.stack은 Stacking이라고 불리며, 무언가를 쌓는다는 뜻이다.\n",
    "\n",
    "즉, Stacking은 순차적으로 벡터를 쌓는 작업을 하는 것이다.\n",
    "\n",
    "아래 코드는 같은 작업을 한 것이지만, torch.cat을 사용했을때 더 복잡해진 모습을 확인 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# Stacking\n",
    "x = torch.FloatTensor([1, 4])\n",
    "y = torch.FloatTensor([2, 5])\n",
    "z = torch.FloatTensor([3, 6])\n",
    "\n",
    "print(torch.stack([x, y, z]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate\n",
    "print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking은 추가적으로 dim을 인자로 줄 수 있다.\n",
    "\n",
    "예를 들어 dim=1은 두번째 차원이 증가하도록 Stack하라는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack([x, y, z], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 산술연산\n",
    "\n",
    "Tensor 행렬 곱(matrix muliplication)\n",
    "\n",
    "Tensor 요소별 곱(element-wise product)\n",
    "\n",
    "*행렬 곱과 요소별 곱의 차이는 행렬 곲은 행렬 곱셈이지만, 요소별 곱셈은 같은 위치에 위치해 있는 원소끼리 곱하는 것이다.\n",
    "\n",
    "Tensor 요소 합\n",
    "\n",
    "Tensor에 특정 수 연산(바꿔치기 연산이라 부름)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# 두 텐서 간의 행렬 곱(matrix multiplication)을 계산합니다. y1, y2, y3은 모두 같은 값을 갖습니다.\n",
    "y1 = tensor @ tensor.T # tensor와 tensor의 전치행렬 곱\n",
    "y2 = tensor.matmul(tensor.T) # tensor와 tensor의 전치행렬 곱\n",
    "\n",
    "y3 = torch.rand_like(tensor) # 행렬 곱 결과를 저장할 tensor 생성\n",
    "torch.matmul(tensor, tensor.T, out=y3) # tensor와 tensor 전치행렬을 곱하여 결과를 y3에 저장\n",
    "\n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 요소별 곱(element-wise product)을 계산합니다. z1, z2, z3는 모두 같은 값을 갖습니다.\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "\n",
    "print(z1)\n",
    "print(z2)\n",
    "print(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.)\n",
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# 단일요소(single-element)의 모든 값을 하나로 집계하여 요소가 하나인 텐서로 만듬\n",
    "# item()을 사용하여 숫자 값으로 변환해야지만 사용이 가능\n",
    "agg = tensor.sum()\n",
    "print(agg)\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# tensor에 특정 수를 연산하는 것 바꿔치기(in-place)연산이라 부르며, 기록이 즉시 삭제되어 도함수 계산에 문제가 될 수 있음.\n",
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뷰(View) - 원소의 수를 유지하면서 텐서의 크기를 변경하는 것\n",
    "\n",
    "Tensor의 View는 NumPy의 Reshape와 같은 역할을 한다,\n",
    "\n",
    "즉, 특정 차원의 Tensor를 다른 차원으로 변경할때 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[[0, 1, 2],\n",
    "               [3, 4, 5]],\n",
    "              [[6, 7, 8],\n",
    "               [9, 10, 11]]])\n",
    "ft = torch.FloatTensor(t)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 차원 변경\n",
    "\n",
    "위의 Tensor는 3차원의 Tensor이다. 이를 2차원으로 변경시켜 볼 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view([-1, 3])) # ft라는 텐서를 (?, 3)의 크기로 변경\n",
    "print(ft.view([-1, 3]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit.view([-1, 3])에서 -1은 첫번째 차원은 Pytorch가 알아서 설정하라는 의미이고, 두번째 차원은 3으로 하라는 이야기이다.\n",
    "\n",
    "결과적으로 (4,3)의 크기를 가지는 Tensor로 바꾸었다.\n",
    "\n",
    "이때, 기본적인 규칙이 있는데 변경 전과 변경 후의 Tensor 안에 원소 개수가 유지되어야한다는 것이다.\n",
    "\n",
    "그렇기 때문에 앞서 (2,2,3)에서 (4,3)으로 변경된 이유는 해당 차원의 곱의 결과가 같아야하기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeeze - 1인 차원을 제거\n",
    "\n",
    "스퀴즈는 차원이 1인 경우에 해당 차원을 제거하는 것이다.\n",
    "\n",
    "예를들어 (2,1,3)이라는 차원이 있을때 가운데 있는 1을 제거하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.FloatTensor([[0], [1], [2]])\n",
    "print(ft)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.squeeze())\n",
    "print(ft.squeeze().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsqueeze - 특정 위치에 1인 차원을 추가\n",
    "\n",
    "Squeeze가 1인 차원을 제거하는 것인 반면 Unsqueeze는 특정위치에 1인 차원을 추가하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.Tensor([0, 1, 2])\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(0)) # 인덱스가 0부터 시작하므로 0은 첫번째 차원을 의미한다.\n",
    "print(ft.unsqueeze(0).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
