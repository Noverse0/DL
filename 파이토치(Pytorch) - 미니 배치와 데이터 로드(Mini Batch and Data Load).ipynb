{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Batch and Data Load\n",
    "\n",
    "우리가 훈련시키는 데이터는 매우 큰 경우가 많고 그에 따라서 많은 계산량을 필요로 한다.\n",
    "\n",
    "그렇기 때문에 우리는 전체 데이터를 한번에 학습시키는 것이 아니라 일정 단위로 나누어서 학습을 하는 방법을 선택하게 된다.\n",
    "\n",
    "미니 배치(Mini Batch)는 앞서 말한 방식에서 데이터를 일정 단위로 나눌때 사용되는 단위이다.\n",
    "\n",
    "전체 데이터를 미니 배치로 나눈다음에 이 미니 배치 수 만큼 학습을 완료하면 1 Epoch가 끝이 난 것이다.\n",
    "\n",
    "미니 배치의 크기를 어떻게 하는가에 대한 문제가 있는데 미니 배치의 크기를 배치 크기(Batch Size)라고 한다.\n",
    "\n",
    "전체 데이터를 미니 배치로 나누어 1개의 미니배치를 학습하는 것이 이터레이션(Iteration)이라고 한다.\n",
    "\n",
    "예를들어 2000개의 데이터를 배치 크기 200으로 나누면 총 10개의 미니 배치로 나누어진다.\n",
    "\n",
    "따라서 10번의 이터레이션이 진행될 것이다. 즉, 가중치와 편향이 10번 업데이트가 이루어 진다는 것을 의미한다. \n",
    "\n",
    "미니 배치로 데이터를 나누어 학습하는 것을 '미니 배치 경사 하강법'이라고 하고, 전체 데이터를 한 번에 하는 것을 '배치 경사 하강법'이라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드하기\n",
    "\n",
    "PyTorch에서는 데이터를 로드하기 위해서 Dataset과 DataLoader를 제공한다.\n",
    "\n",
    "기본적으로는 Dataset을 정의하고 DataLoader에 전달하는 것이다.\n",
    "\n",
    "이를 이용해서 미니 배치 학습, 데이터 셔플, 병렬 처리를 수행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
    "\n",
    "dataset = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader를 이용할때는 Dataset과 batch size를 정의해주어야한다.\n",
    "\n",
    "보통 batch size는 2의 배수를 상용한다.\n",
    "\n",
    "shuffle은 Epoch를 진행할때마다 데이터의 순서를 Random하게 섞는 것을 의미한다.\n",
    "\n",
    "똑같은 데이터여도 데이터의 배치를 Random하게 섞으면 더 잘 학습이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(3,1) #3가지 값을 입력 받아서 1개의 값을 출력하는 다중 선형 회귀 모델\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) #SGD를 이용하여, Learning rate는 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 34233.855469\n",
      "Epoch    0/20 Batch 2/3 Cost: 13817.115234\n",
      "Epoch    0/20 Batch 3/3 Cost: 5960.549805\n",
      "Epoch    1/20 Batch 1/3 Cost: 730.721313\n",
      "Epoch    1/20 Batch 2/3 Cost: 437.575684\n",
      "Epoch    1/20 Batch 3/3 Cost: 110.303520\n",
      "Epoch    2/20 Batch 1/3 Cost: 5.684276\n",
      "Epoch    2/20 Batch 2/3 Cost: 25.150545\n",
      "Epoch    2/20 Batch 3/3 Cost: 20.171661\n",
      "Epoch    3/20 Batch 1/3 Cost: 4.058476\n",
      "Epoch    3/20 Batch 2/3 Cost: 11.481176\n",
      "Epoch    3/20 Batch 3/3 Cost: 8.568769\n",
      "Epoch    4/20 Batch 1/3 Cost: 8.175518\n",
      "Epoch    4/20 Batch 2/3 Cost: 4.682809\n",
      "Epoch    4/20 Batch 3/3 Cost: 11.290629\n",
      "Epoch    5/20 Batch 1/3 Cost: 5.878546\n",
      "Epoch    5/20 Batch 2/3 Cost: 8.409691\n",
      "Epoch    5/20 Batch 3/3 Cost: 8.666419\n",
      "Epoch    6/20 Batch 1/3 Cost: 0.360738\n",
      "Epoch    6/20 Batch 2/3 Cost: 18.866890\n",
      "Epoch    6/20 Batch 3/3 Cost: 10.444901\n",
      "Epoch    7/20 Batch 1/3 Cost: 9.642101\n",
      "Epoch    7/20 Batch 2/3 Cost: 7.243816\n",
      "Epoch    7/20 Batch 3/3 Cost: 2.639088\n",
      "Epoch    8/20 Batch 1/3 Cost: 7.982546\n",
      "Epoch    8/20 Batch 2/3 Cost: 1.502467\n",
      "Epoch    8/20 Batch 3/3 Cost: 20.308544\n",
      "Epoch    9/20 Batch 1/3 Cost: 5.506280\n",
      "Epoch    9/20 Batch 2/3 Cost: 13.296759\n",
      "Epoch    9/20 Batch 3/3 Cost: 13.107569\n",
      "Epoch   10/20 Batch 1/3 Cost: 11.068458\n",
      "Epoch   10/20 Batch 2/3 Cost: 4.852242\n",
      "Epoch   10/20 Batch 3/3 Cost: 3.131781\n",
      "Epoch   11/20 Batch 1/3 Cost: 0.755255\n",
      "Epoch   11/20 Batch 2/3 Cost: 17.807590\n",
      "Epoch   11/20 Batch 3/3 Cost: 10.721088\n",
      "Epoch   12/20 Batch 1/3 Cost: 2.713362\n",
      "Epoch   12/20 Batch 2/3 Cost: 11.415572\n",
      "Epoch   12/20 Batch 3/3 Cost: 9.632188\n",
      "Epoch   13/20 Batch 1/3 Cost: 10.007609\n",
      "Epoch   13/20 Batch 2/3 Cost: 8.706565\n",
      "Epoch   13/20 Batch 3/3 Cost: 6.348192\n",
      "Epoch   14/20 Batch 1/3 Cost: 4.422946\n",
      "Epoch   14/20 Batch 2/3 Cost: 17.303089\n",
      "Epoch   14/20 Batch 3/3 Cost: 3.704932\n",
      "Epoch   15/20 Batch 1/3 Cost: 0.735436\n",
      "Epoch   15/20 Batch 2/3 Cost: 12.143666\n",
      "Epoch   15/20 Batch 3/3 Cost: 10.570932\n",
      "Epoch   16/20 Batch 1/3 Cost: 10.621038\n",
      "Epoch   16/20 Batch 2/3 Cost: 4.507910\n",
      "Epoch   16/20 Batch 3/3 Cost: 5.320943\n",
      "Epoch   17/20 Batch 1/3 Cost: 5.513970\n",
      "Epoch   17/20 Batch 2/3 Cost: 8.252976\n",
      "Epoch   17/20 Batch 3/3 Cost: 8.659143\n",
      "Epoch   18/20 Batch 1/3 Cost: 9.979591\n",
      "Epoch   18/20 Batch 2/3 Cost: 4.036917\n",
      "Epoch   18/20 Batch 3/3 Cost: 13.706807\n",
      "Epoch   19/20 Batch 1/3 Cost: 6.636245\n",
      "Epoch   19/20 Batch 2/3 Cost: 14.586987\n",
      "Epoch   19/20 Batch 3/3 Cost: 2.874395\n",
      "Epoch   20/20 Batch 1/3 Cost: 9.689045\n",
      "Epoch   20/20 Batch 2/3 Cost: 4.194235\n",
      "Epoch   20/20 Batch 3/3 Cost: 13.547850\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "  for batch_idx, samples in enumerate(dataloader):\n",
    "    # print(batch_idx)\n",
    "    # print(samples)\n",
    "    x_train, y_train = samples\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 계산\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "        cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[155.3204]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [73, 80, 75]를 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) \n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset\n",
    "\n",
    "우리는 위에서 Dataset을 정의한 다음 DataLoader를 이용하여 Mini Batch 방법을 수행하였다.\n",
    "\n",
    "이번에는 직접 Dataset을 만들어 사용해 볼 것이다.\n",
    "\n",
    "직접 Dataset을 만들기 위해서는 Class를 사용해야하는데, 우리는 3가지 define을 사용할 것이다.\n",
    "\n",
    "'__init__', '__len__'과 '__getitem__'이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 상속\n",
    "class CustomDataset(torch.utils.data.Dataset): \n",
    "  def __init__(self):\n",
    "    self.x_data = [[73, 80, 75],\n",
    "                   [93, 88, 93],\n",
    "                   [89, 91, 90],\n",
    "                   [96, 98, 100],\n",
    "                   [73, 66, 70]]\n",
    "    self.y_data = [[152], [185], [180], [196], [142]]\n",
    "\n",
    "  # 총 데이터의 개수를 리턴\n",
    "  def __len__(self): \n",
    "    return len(self.x_data)\n",
    "\n",
    "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "  def __getitem__(self, idx): \n",
    "    x = torch.FloatTensor(self.x_data[idx])\n",
    "    y = torch.FloatTensor(self.y_data[idx])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 10097.804688\n",
      "Epoch    0/20 Batch 2/3 Cost: 2283.451416\n",
      "Epoch    0/20 Batch 3/3 Cost: 507.059875\n",
      "Epoch    1/20 Batch 1/3 Cost: 297.864960\n",
      "Epoch    1/20 Batch 2/3 Cost: 141.844543\n",
      "Epoch    1/20 Batch 3/3 Cost: 20.542185\n",
      "Epoch    2/20 Batch 1/3 Cost: 15.037828\n",
      "Epoch    2/20 Batch 2/3 Cost: 8.075519\n",
      "Epoch    2/20 Batch 3/3 Cost: 0.063480\n",
      "Epoch    3/20 Batch 1/3 Cost: 5.601187\n",
      "Epoch    3/20 Batch 2/3 Cost: 2.735129\n",
      "Epoch    3/20 Batch 3/3 Cost: 1.664595\n",
      "Epoch    4/20 Batch 1/3 Cost: 7.428864\n",
      "Epoch    4/20 Batch 2/3 Cost: 3.328489\n",
      "Epoch    4/20 Batch 3/3 Cost: 0.627107\n",
      "Epoch    5/20 Batch 1/3 Cost: 3.193416\n",
      "Epoch    5/20 Batch 2/3 Cost: 1.095299\n",
      "Epoch    5/20 Batch 3/3 Cost: 9.350142\n",
      "Epoch    6/20 Batch 1/3 Cost: 3.171443\n",
      "Epoch    6/20 Batch 2/3 Cost: 5.904304\n",
      "Epoch    6/20 Batch 3/3 Cost: 1.769514\n",
      "Epoch    7/20 Batch 1/3 Cost: 2.959316\n",
      "Epoch    7/20 Batch 2/3 Cost: 4.274165\n",
      "Epoch    7/20 Batch 3/3 Cost: 1.429538\n",
      "Epoch    8/20 Batch 1/3 Cost: 6.805875\n",
      "Epoch    8/20 Batch 2/3 Cost: 3.574557\n",
      "Epoch    8/20 Batch 3/3 Cost: 0.690956\n",
      "Epoch    9/20 Batch 1/3 Cost: 4.970878\n",
      "Epoch    9/20 Batch 2/3 Cost: 2.419390\n",
      "Epoch    9/20 Batch 3/3 Cost: 1.619429\n",
      "Epoch   10/20 Batch 1/3 Cost: 3.244032\n",
      "Epoch   10/20 Batch 2/3 Cost: 4.020617\n",
      "Epoch   10/20 Batch 3/3 Cost: 1.744394\n",
      "Epoch   11/20 Batch 1/3 Cost: 0.273157\n",
      "Epoch   11/20 Batch 2/3 Cost: 5.343218\n",
      "Epoch   11/20 Batch 3/3 Cost: 4.728515\n",
      "Epoch   12/20 Batch 1/3 Cost: 2.416982\n",
      "Epoch   12/20 Batch 2/3 Cost: 3.020668\n",
      "Epoch   12/20 Batch 3/3 Cost: 7.773168\n",
      "Epoch   13/20 Batch 1/3 Cost: 3.530616\n",
      "Epoch   13/20 Batch 2/3 Cost: 2.387966\n",
      "Epoch   13/20 Batch 3/3 Cost: 6.178110\n",
      "Epoch   14/20 Batch 1/3 Cost: 1.840215\n",
      "Epoch   14/20 Batch 2/3 Cost: 7.503317\n",
      "Epoch   14/20 Batch 3/3 Cost: 2.061496\n",
      "Epoch   15/20 Batch 1/3 Cost: 0.446374\n",
      "Epoch   15/20 Batch 2/3 Cost: 3.563155\n",
      "Epoch   15/20 Batch 3/3 Cost: 7.626163\n",
      "Epoch   16/20 Batch 1/3 Cost: 3.572958\n",
      "Epoch   16/20 Batch 2/3 Cost: 4.011487\n",
      "Epoch   16/20 Batch 3/3 Cost: 2.804664\n",
      "Epoch   17/20 Batch 1/3 Cost: 2.560488\n",
      "Epoch   17/20 Batch 2/3 Cost: 3.560911\n",
      "Epoch   17/20 Batch 3/3 Cost: 3.644734\n",
      "Epoch   18/20 Batch 1/3 Cost: 0.761189\n",
      "Epoch   18/20 Batch 2/3 Cost: 4.908943\n",
      "Epoch   18/20 Batch 3/3 Cost: 4.185076\n",
      "Epoch   19/20 Batch 1/3 Cost: 3.921267\n",
      "Epoch   19/20 Batch 2/3 Cost: 2.328156\n",
      "Epoch   19/20 Batch 3/3 Cost: 6.082151\n",
      "Epoch   20/20 Batch 1/3 Cost: 3.697572\n",
      "Epoch   20/20 Batch 2/3 Cost: 2.461065\n",
      "Epoch   20/20 Batch 3/3 Cost: 3.649280\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "  for batch_idx, samples in enumerate(dataloader):\n",
    "    # print(batch_idx)\n",
    "    # print(samples)\n",
    "    x_train, y_train = samples\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 계산\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "        cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[153.0042]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [73, 80, 75]를 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) \n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
