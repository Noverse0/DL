{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형회귀(Linear Regression)\n",
    "\n",
    "선형회귀 개념에 대한 정리는 제 Notion에 정리가 되어 있습니다.\n",
    "\n",
    "https://noversezero.notion.site/9884bad5e8d14d4aabaf38dad2dd2e25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기본세팅\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n",
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "print(x_train)\n",
    "print(x_train.shape)\n",
    "\n",
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 Y = Wx + B로 되어 있는 Linear Regression을 진행할 것이다.\n",
    "\n",
    "우리의 최종 목표는 비용함수(Cost Function)이 최소화되는 직선을 찾는 것이다.\n",
    "\n",
    "그러기 위해서는 초기 Weight 값과 Bias 값을 설정해야하기 때문에 0으로 초기화 시킬 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 가중치 W와 편향 b를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시함.\n",
    "W = torch.zeros(1, requires_grad=True) \n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# 가중치 W를 출력\n",
    "print(W) \n",
    "\n",
    "# 편향 b를 출력\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 우리의 방정식은 Y = 0x + 0 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가설  세우기\n",
    "\n",
    "우리의 가설은 가중치 하나로 이루어져 있는 Linear Regression Model이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비용함수 선언하기\n",
    "\n",
    "우리의 비용함수는 MSE(Mean Squared Error)이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 앞서 배운 torch.mean으로 평균을 구한다.\n",
    "cost = torch.mean((hypothesis - y_train) ** 2) \n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer 설정\n",
    "\n",
    "확률적 경사하강법(SGD, Stochastic Gradient Descent)은 매 스텝(step)에서 딱 1개의 샘플을 무작위로 선택하고 그에 대한 gradient를 계산하는 것이다.\n",
    "\n",
    "매 스텝에서 1개의 샘플을 무작위로 선정하기 때문에 샘플 전체를 사용하는 것보다 더 빠르게 진행 될 수 있다.\n",
    "\n",
    "따라서 매우 큰 데이터에 적용하기 좋다.\n",
    "\n",
    "이와 반대로 모든 샘플을 이용하는 경사하강법은 배치 경사하강법이다.\n",
    "\n",
    "위에 링크를 걸어 놓은 내 Notion에 있는 gradient 계산은 배치 경사하강법을 위한 gradient 계산이라고 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01) #이때 lr은 Learning rate이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 Gradient descent를 구현하기 위해서는 3가지 함수를 사용할 것이다.\n",
    "\n",
    "optimizer.zero_grad() : gradient를 0으로 초기화\n",
    "\n",
    "cost.backward() : 비용 함수를 미분하여 gradient 계산\n",
    "\n",
    "optimizer.step() : W와 b를 업데이트\n",
    "\n",
    "gradient 초기화 -> gradient 계산 -> W와 b Update를 정해진 횟수만큼 반복할 것이다.\n",
    "\n",
    "이때 왜 gradient를 초기화해야하냐면, Pytorch는 Gradient를 계속 누적시키는 특징이 있다.\n",
    "\n",
    "그렇기 때문에 gradient를 0으로 초기화하지 않으면, 새로 구한 gradient와 이전의 gradient의 합으로 W와 b를 Update하게 된다.\n",
    "\n",
    "따라서 gradient를 초기화해야지 다음 gradient로 Update가 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1999 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch  100/1999 W: 1.746, b: 0.578 Cost: 0.048171\n",
      "Epoch  200/1999 W: 1.800, b: 0.454 Cost: 0.029767\n",
      "Epoch  300/1999 W: 1.843, b: 0.357 Cost: 0.018394\n",
      "Epoch  400/1999 W: 1.876, b: 0.281 Cost: 0.011366\n",
      "Epoch  500/1999 W: 1.903, b: 0.221 Cost: 0.007024\n",
      "Epoch  600/1999 W: 1.924, b: 0.174 Cost: 0.004340\n",
      "Epoch  700/1999 W: 1.940, b: 0.136 Cost: 0.002682\n",
      "Epoch  800/1999 W: 1.953, b: 0.107 Cost: 0.001657\n",
      "Epoch  900/1999 W: 1.963, b: 0.084 Cost: 0.001024\n",
      "Epoch 1000/1999 W: 1.971, b: 0.066 Cost: 0.000633\n",
      "Epoch 1100/1999 W: 1.977, b: 0.052 Cost: 0.000391\n",
      "Epoch 1200/1999 W: 1.982, b: 0.041 Cost: 0.000242\n",
      "Epoch 1300/1999 W: 1.986, b: 0.032 Cost: 0.000149\n",
      "Epoch 1400/1999 W: 1.989, b: 0.025 Cost: 0.000092\n",
      "Epoch 1500/1999 W: 1.991, b: 0.020 Cost: 0.000057\n",
      "Epoch 1600/1999 W: 1.993, b: 0.016 Cost: 0.000035\n",
      "Epoch 1700/1999 W: 1.995, b: 0.012 Cost: 0.000022\n",
      "Epoch 1800/1999 W: 1.996, b: 0.010 Cost: 0.000013\n",
      "Epoch 1900/1999 W: 1.997, b: 0.008 Cost: 0.000008\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 1999 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 출력된 결과를 보면 Cost가 계속 줄어드는 것을 볼 수 있다.\n",
    "\n",
    "처음에는 18.666이 였지만 1900 반복했을때는 0.000008까지 줄어드는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자동 미분(Autograd)\n",
    "\n",
    "PyTorch는 우리가 만드는 Tensor에 requires_grad = True를 입력하면 자동으로 gradient를 계산해준다.\n",
    "\n",
    "모델이 복잡해질수록 우리가 Gradient descent를 직접 코딩하는 것은 매우 복잡한 일이 될 것이다.\n",
    "\n",
    "하지만 PyTorch의 자동 미분 기능이 있기 때문에 우리는 backward()함수를 통해 미분을 쉽게 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 8.0\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(2.0, requires_grad=True) #requires_grad 옵션을 통해 자동미분 기능을 갖고 있는 tensor를 생성하였다.\n",
    "\n",
    "y = w**2\n",
    "z = 2*y + 5 \n",
    "# w의 제곱에 2를 곱하고 5를 더하는 방정식을 생성하였다. \n",
    "\n",
    "#이때 우리가 직접 미분하는 코드를 코딩할 필요가 없고 backward() 함수를 사용하면 손쉽게 미분이 가능하다.\n",
    "z.backward()\n",
    "# 이제 미분을 한 결과를 grad() 함수를 통해 출력하면, 미분한 값이 나온다.\n",
    "print('수식을 w로 미분한 값 : {}'.format(w.grad))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
