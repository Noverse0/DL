{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cfaf1d3130>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Module로 구현하는 선형회귀\n",
    "\n",
    "앞선 선형회귀와 다중선형회귀에서는 직접 가설과 비용함수를 코딩했지만, PyTorch에서 제공하는 함수들을 이용하여 더 쉽게 선형회귀 모델을 만들 수 있다.\n",
    "\n",
    "예를 들어 PyTorch에는 선형 회귀 모델이 nn.Linear()라는 함수로, 또 평균 제곱오차가 nn.functional.mse_loss()라는 함수로 구현되어져 있다.\n",
    "\n",
    "## 단순 선형회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 선언 및 초기화. 단순 선형 회귀이므로 input_dim=1, output_dim=1.\n",
    "model = nn.Linear(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear(입력 차원,출력 차원) 함수는 입력 차원과 출력 차원을 입력 받는다.\n",
    "\n",
    "nn.Linear(1,1)은 1개의 입력 값을 받아서 1개의 값을 출력한다는 뜻이다.\n",
    "\n",
    "다중 선형 회귀 일때는 1가지가 아니고 2가지 이상의 입력 값을 받게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4414], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".paramters() 함수는 Model의 가중치 W와 편향 b를 출력한다.\n",
    "\n",
    "결과 값을 보면 앞에 있는 값이 가중치의 값이고, 두번째로나오는 값이 편향의 값이다.\n",
    "\n",
    "현재 가중치와 편향은 둘다 Random하게 설정되어 있다. 이를 학습을 해보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 13.103541\n",
      "Epoch  100/2000 Cost: 0.002791\n",
      "Epoch  200/2000 Cost: 0.001724\n",
      "Epoch  300/2000 Cost: 0.001066\n",
      "Epoch  400/2000 Cost: 0.000658\n",
      "Epoch  500/2000 Cost: 0.000407\n",
      "Epoch  600/2000 Cost: 0.000251\n",
      "Epoch  700/2000 Cost: 0.000155\n",
      "Epoch  800/2000 Cost: 0.000096\n",
      "Epoch  900/2000 Cost: 0.000059\n",
      "Epoch 1000/2000 Cost: 0.000037\n",
      "Epoch 1100/2000 Cost: 0.000023\n",
      "Epoch 1200/2000 Cost: 0.000014\n",
      "Epoch 1300/2000 Cost: 0.000009\n",
      "Epoch 1400/2000 Cost: 0.000005\n",
      "Epoch 1500/2000 Cost: 0.000003\n",
      "Epoch 1600/2000 Cost: 0.000002\n",
      "Epoch 1700/2000 Cost: 0.000001\n",
      "Epoch 1800/2000 Cost: 0.000001\n",
      "Epoch 1900/2000 Cost: 0.000000\n",
      "Epoch 2000/2000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n",
    "\n",
    "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train) #우리의 모델은 nn.Linear(1,1)로 정의 하였다. 이 모델에 train data를 넣어서 학습을 진행한다.\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward() # backward 연산\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 4일 때의 예측값 : tensor([[7.9989]], grad_fn=<AddmmBackward0>)\n",
      "[Parameter containing:\n",
      "tensor([[1.9994]], requires_grad=True), Parameter containing:\n",
      "tensor([0.0014], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 4를 선언\n",
    "new_var =  torch.FloatTensor([[4.0]]) \n",
    "# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) # forward 연산\n",
    "# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\n",
    "print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y) \n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost를 계산할때 F.mse_loss(prediction, y_train)를 통해 cost function인 mse를 계산하였다.\n",
    "\n",
    "이전에는 직접 cost function의 계산을 코딩해주었지만, Pytorch에서 제공하는 함수를 이용하여 코딩하였다.\n",
    "\n",
    "우리는 forward 연산과 backward 연산에 대해서 명확히 정의해야한다.\n",
    "\n",
    "forward 연산은 제공한 train data를 이용하여, 우리의 예측 값을 추정하는 것이다.\n",
    "\n",
    "따라서 코드에서 prediction = model(x_train)과 pred_y = model(new_var)이다.\n",
    "\n",
    "반면, 학습 과정에서 Cost Function을 미분하여, 가중치와 편향을 Update해나가는 것은 backward 연산이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다중 선형 회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.1188,  0.2937,  0.0803]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0707], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 모델을 선언 및 초기화. 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
    "model = nn.Linear(3,1)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 22642.416016\n",
      "Epoch  100/2000 Cost: 4.976951\n",
      "Epoch  200/2000 Cost: 4.733481\n",
      "Epoch  300/2000 Cost: 4.502824\n",
      "Epoch  400/2000 Cost: 4.284275\n",
      "Epoch  500/2000 Cost: 4.077209\n",
      "Epoch  600/2000 Cost: 3.881021\n",
      "Epoch  700/2000 Cost: 3.695124\n",
      "Epoch  800/2000 Cost: 3.519025\n",
      "Epoch  900/2000 Cost: 3.352130\n",
      "Epoch 1000/2000 Cost: 3.194023\n",
      "Epoch 1100/2000 Cost: 3.044178\n",
      "Epoch 1200/2000 Cost: 2.902206\n",
      "Epoch 1300/2000 Cost: 2.767665\n",
      "Epoch 1400/2000 Cost: 2.640176\n",
      "Epoch 1500/2000 Cost: 2.519374\n",
      "Epoch 1600/2000 Cost: 2.404892\n",
      "Epoch 1700/2000 Cost: 2.296402\n",
      "Epoch 1800/2000 Cost: 2.193589\n",
      "Epoch 1900/2000 Cost: 2.096157\n",
      "Epoch 2000/2000 Cost: 2.003828\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    # model(x_train)은 model.forward(x_train)와 동일함.\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward()\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[152.9719]], grad_fn=<AddmmBackward0>)\n",
      "[Parameter containing:\n",
      "tensor([[0.6294, 0.6883, 0.6936]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0603], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [73, 80, 75]를 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) \n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) \n",
    "print(list(model.parameters()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
